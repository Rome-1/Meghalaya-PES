method: bayes  # Bayesian optimization
early_terminate:
  type: hyperband  # Use Hyperband for early stopping
  max_iter: 40  # formerly 81; Maximum iterations (for example, max number of epochs)
  s: 4  # Hyperband aggressiveness factor (higher value means more aggressive stopping)
metric:
  name: validation_AUC  # Metric to optimize
  goal: minimize  # Minimize the validation loss

parameters:
  region:
    value: "meghalaya_only"  # Set fixed region

  kernel_size:
    values:
      - [ [3, 3], [3, 3], [3, 3], [3, 3] ]
      - [ [5, 5], [5, 5], [3, 3], [3, 3] ]
      - [ [7, 7], [7, 7], [3, 3], [3, 3] ]
      - [ [9, 9], [7, 7], [5, 5], [3, 3] ]

  stride:
    value: 
      - [2, 2]
      - [1, 1]
      - [1, 1]
      - [1, 1]

  padding:
    value: [0, 0, 0, 0]

  size:
    values: [25, 29, 35, 39, 45, 49, 55]

  dropout:
    distribution: uniform
    min: 0.1
    max: 0.6

  levels:
    values: [6, 8, 10, 12, 14]

  batch_size:
    values: [128, 256, 512, 1024, 2056]

  hidden_dim1:
    values: [32, 64, 128, 256]

  hidden_dim2:
    values: [32, 64, 128, 256]

  hidden_dim3:
    values: [32, 64, 128, 256]

  hidden_dim3:
    values: [32, 64, 128, 256]

  lr:
    distribution: log_uniform_values  # Log uniform distribution for learning rate
    min: 0.000005
    max: 0.01

  weight_decay:
    value: 0

  n_splits:
    value: 5

  AUC:
    value: true

  BCE_Wloss:
    value: false

  FNcond:
    value: false

  n_epochs:
    values: [10] # epochs

  patience:
    value: 7

  training_time:
    values: [21] # hours

  pos_weight:
    values: [2, 5, 10]

  train_times:
    values: [1, 2, 4, 8]

  test_times:
    values: [1, 2, 4, 8]

  w:
    value: 10

  stop_batch:
    value: null

  print_batch:
    value: 500

  start_year:
    value: 20

  end_year:
    value: 22  

  nightlight_log:
    values: [True, False]
